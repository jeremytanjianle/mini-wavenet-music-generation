{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c00769",
   "metadata": {},
   "source": [
    "## Music Generation with a mini wavenet\n",
    "Wavenet reads raw audio and generates music. For a micro experiment with limited data, we can reduce the difficulty of the training task by directly sampling the notes of the music directly to simplify the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75d42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "while 'notebook' in os.getcwd():\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1a083",
   "metadata": {},
   "source": [
    "### 1. Read data\n",
    "Data is retrieved from: http://www.piano-midi.de/schub.html  \n",
    "The midi files are read and saved as a numpy array of the notes.  \n",
    "\n",
    "Warning: the cell below may take a while. Instead, you can read the numpy array directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f8b0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D5', 'B-2', 'D4', ..., '5.10', '10.2.5', '5.10'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import converter, instrument, note, chord, stream, corpus, midi\n",
    "\n",
    "#defining function to read MIDI files\n",
    "def read_midi(file, verbose=False):\n",
    "    \n",
    "    if verbose: print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)\n",
    "\n",
    "#specify the path\n",
    "path='schubert/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])\n",
    "\n",
    "with open('notes_array.npy', 'wb') as f:\n",
    "    np.save(f, notes_array)\n",
    "    \n",
    "# with open('notes_array.npy', 'wb') as f:\n",
    "#     notes_array = np.load(f)\n",
    "\n",
    "display(notes_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ed002",
   "metadata": {},
   "source": [
    "### 2. EDA: what do the unique notes look like?\n",
    "There seems to be a \"power law\" with the notes, that is, a small number of notes dominate the songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a23b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([221.,  40.,  31.,  13.,   8.,   3.,   7.,   9.,   7.,   6.]),\n",
       " array([1.0000e+00, 1.6170e+02, 3.2240e+02, 4.8310e+02, 6.4380e+02,\n",
       "        8.0450e+02, 9.6520e+02, 1.1259e+03, 1.2866e+03, 1.4473e+03,\n",
       "        1.6080e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn8AAAJdCAYAAABDKhHGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAABYlAAAWJQFJUiTwAAAkl0lEQVR4nO3dfbhtVV0v8O8vjoGgIHotLbodNFHKNMVXfC4i3nx8lxJv3J4MLS0rX1D0ar7U0WtFia/o1dISg26oWJqJLyUiKpYKmXlFEeFoJoiIgMABRcf9Y84tu8Ve5+y9z9577b3H5/M865lnjTnHnGOsudY63z3WfKnWWgAA6MMPzboBAACsHeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICObJl1A9aLqrooyb5Jts+4KQAAu7I1yVWttQOXWlH4u9G+N7/5zW998MEH33rWDQEA2JnzzjsvO3bsWFZd4e9G2w8++OBbn3POObNuBwDATh1yyCE599xzty+nrmP+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHdky6wb0Zuvz3jPrJqyY7cc/YtZNAACWyMgfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICO7Hb4q6rbVNWTqupvq+qCqtpRVVdW1Uer6terasFtVNWhVXV6VV0+1vlMVR1bVXvsZFuPrKozx/VfXVX/XFXH7G4fAAB6sWUF1vG4JK9PcnGSDyX5SpIfTfKLSd6U5GFV9bjWWpurUFWPSfKOJNcleWuSy5M8KskrkzxgXOd/UlVPTXJikm8mOSXJd5IcleSkqvrZ1tqzV6AvAACb2kqEv/OTPDrJe1pr358rrKrnJ/lEksdmCILvGMv3TfLGJN9Lcnhr7VNj+YuSnJHkqKo6urV26rx1bU1yQoaQeK/W2vax/CVJPpnkuKp6R2vt4yvQHwCATWu3f/ZtrZ3RWnv3/OA3ll+S5A3j08PnzToqyW2TnDoX/Mblr0vywvHpb01s5teS7JnktXPBb6zzrSR/OD59yu71BABg81vtEz6+O05vmFd2xDh93wLLn5Xk2iSHVtWei6zz3ollAACYYiV+9l1QVW1J8qvj0/mh7c7j9PzJOq21G6rqoiQ/k+QOSc5bRJ2Lq+qaJAdU1d6ttWt30a5zpsy6y87qAQBsBqs58nd8krsmOb219v555fuN0yun1Jsrv9Uy6uw3ZT4AAFmlkb+qenqS45J8PsnjV2Mby9VaO2Sh8nFE8J5r3BwAgDW14iN/4yVZXp3kc0ke1Fq7fGKRXY3SzZVfsYw600YGAQDICoe/qjo2w7X4Ppsh+F2ywGJfGKcHLVB/S5IDM5wgcuEi69w+yT5Jvrqr4/0AAHq3YuGvqp6b4SLNn84Q/C6dsugZ4/ShC8w7LMneSc5urV2/yDoPm1gGAIApViT8jRdoPj7JOUke3Fq7bCeLn5bksiRHV9W95q1jryQvHZ++fqLOm5Ncn+Sp4wWf5+rsn+T549M3BACAndrtEz7Ge+u+JMMdOz6S5OlVNbnY9tbaSUnSWruqqp6cIQSeWVWnZrhzx6MzXNLltAy3fPuB1tpFVfWcJK9J8qmqemtuvL3bAUle7u4eAAC7thJn+x44TvdIcuyUZT6c5KS5J621d1bVA5O8IMPt3/ZKckGSZyV5zfz7AM+rc2JVbU/y7AzXD/yhDCeVvLC19pYV6AcAwKa32+GvtbYtybZl1PtYkocvsc67k7x7qdsCAGCw2rd3AwBgHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjKxL+quqoqjqxqj5SVVdVVauqU6Ysu3WcP+1x6k62c0xVfaKqrq6qK6vqzKp65Er0AQCgB1tWaD0vTHL3JFcn+WqSuyyizr8meecC5Z9daOGqOiHJceP635jkh5McneTdVfW01tprl95sAIC+rFT4e2aGUHZBkgcm+dAi6ny6tbZtMSuvqkMzBL8vJbl3a+1bY/nLkpyT5ISq+vvW2valNx0AoB8r8rNva+1DrbUvttbaSqxvAU8Zp38wF/zG7W5P8rokeyZ54iptGwBg05jlCR8/VlW/WVXPH6d328myR4zT9y0w770TywAAMMVK/ey7HD8/Pn6gqs5Mckxr7SvzyvZJ8uNJrm6tXbzAer44Tg9azEar6pwpsxZznCIAwIY2i5G/a5P87ySHJNl/fMwdJ3h4kg+OgW/OfuP0yinrmyu/1Uo3FABgs1nzkb/W2qVJfm+i+KyqekiSjya5b5InJXn1Km3/kIXKxxHBe67GNgEA1ot1c5Hn1toNSd40Pj1s3qy5kb39srC58itWoVkAAJvKugl/o2+M0x/87NtauybJfyS5RVXdfoE6dxqn569y2wAANrz1Fv7uN04vnCg/Y5w+dIE6D5tYBgCAKdY8/FXVPavqJtutqgdnuFh0kkzeGu4N4/QFVbX/vDpbk/xOkuuTvHnlWwsAsLmsyAkfVXVkkiPHp7cbp/evqpPGf1/WWnv2+O9XJLlTVZ2d4a4gSXK33Hidvhe11s6ev/7W2tlV9Yokz0rymao6LcPt3X4pya2TPM3dPQAAdm2lzvb9uSTHTJTdYXwkyZeTzIW/k5P8QpJ7Z/jJ9mZJvp7kbUle21r7yEIbaK0dV1X/lmGk7zeSfD/JuUle1lr7+xXqBwDAprYi4W+8R++2RS7750n+fJnbOSnJScupCwDA+jvhAwCAVST8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHViT8VdVRVXViVX2kqq6qqlZVp+yizqFVdXpVXV5VO6rqM1V1bFXtsZM6j6yqM6vqyqq6uqr+uaqOWYk+AAD0YMsKreeFSe6e5OokX01yl50tXFWPSfKOJNcleWuSy5M8KskrkzwgyeMWqPPUJCcm+WaSU5J8J8lRSU6qqp9trT17hfoCALBprdTPvs9MclCSfZP81s4WrKp9k7wxyfeSHN5a+/XW2nOS/FySjyc5qqqOnqizNckJGULivVprv9Nae2aSuyX5UpLjqur+K9QXAIBNa0XCX2vtQ621L7bW2iIWPyrJbZOc2lr71Lx1XJdhBDG5aYD8tSR7Jnlta237vDrfSvKH49OnLLP5AADdmMUJH0eM0/ctMO+sJNcmObSq9lxknfdOLAMAwBQrdczfUtx5nJ4/OaO1dkNVXZTkZ5LcIcl5i6hzcVVdk+SAqtq7tXbtzjZeVedMmbXT4xQBADaDWYz87TdOr5wyf678Vsuos9+U+QAAZDYjfzPVWjtkofJxRPCea9wcAIA1NYuRv12N0s2VX7GMOtNGBgEAyGzC3xfG6UGTM6pqS5IDk9yQ5MJF1rl9kn2SfHVXx/sBAPRuFuHvjHH60AXmHZZk7yRnt9auX2Sdh00sAwDAFLMIf6cluSzJ0VV1r7nCqtoryUvHp6+fqPPmJNcneep4wee5Ovsnef749A2r1WAAgM1iRU74qKojkxw5Pr3dOL1/VZ00/vuyuduvtdauqqonZwiBZ1bVqRnu3PHoDJd0OS3DLd9+oLV2UVU9J8lrknyqqt6aG2/vdkCSl7fWPr4SfQEA2MxW6mzfn0tyzETZHcZHknw5yQ/uvdtae2dVPTDJC5I8NsleSS5I8qwkr1noTiGttROravu4nl/NMGr5uSQvbK29ZYX6AQCwqa1I+GutbUuybYl1Ppbk4Uus8+4k715KHQAAbjSLY/4AAJgR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOjKz8FdV26uqTXlcMqXOoVV1elVdXlU7quozVXVsVe2x1u0HANiItsx4+1cmedUC5VdPFlTVY5K8I8l1Sd6a5PIkj0ryyiQPSPK4VWslAMAmMevwd0VrbduuFqqqfZO8Mcn3khzeWvvUWP6iJGckOaqqjm6tnbqajQUA2Og2yjF/RyW5bZJT54JfkrTWrkvywvHpb82iYQAAG8msR/72rKpfSfJfk1yT5DNJzmqtfW9iuSPG6fsWWMdZSa5NcmhV7dlau37VWgsAsMHNOvzdLsnJE2UXVdUTW2sfnld253F6/uQKWms3VNVFSX4myR2SnLezDVbVOVNm3WVxTQYA2Lhm+bPvm5M8OEMA3CfJzyb50yRbk7y3qu4+b9n9xumVU9Y1V36rFW8lAMAmMrORv9baiyeKPpvkKVV1dZLjkmxL8gursN1DFiofRwTvudLbAwBYT9bjCR9vGKeHzSubG9nbLwubK79iNRoEALBZrMfw941xus+8si+M04MmF66qLUkOTHJDkgtXt2kAABvbegx/9xun84PcGeP0oQssf1iSvZOc7UxfAICdm0n4q6qDq2qfBcq3Jnnt+PSUebNOS3JZkqOr6l7zlt8ryUvHp69fndYCAGweszrh45eSHFdVZyX5cpJvJ7ljkkck2SvJ6UlOmFu4tXZVVT05Qwg8s6pOzXB7t0dnuAzMaRlu+QYAwE7MKvx9KENou0eG+/Luk+FkjY9muO7fya21Nr9Ca+2dVfXAJC9I8tgMIfGCJM9K8prJ5QEAuKmZhL/xAs4f3uWCN633sSQPX/kWAQD0YT2e8AEAwCoR/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEe2zLoBbFxbn/eeWTdhxWw//hGzbgIArAkjfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOjIllk3ANaDrc97z6ybsCK2H/+IWTcBgHXOyB8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHtsy6AcDK2fq898y6CStm+/GPmHUTADYlI38AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBHhD8AgI4IfwAAHXGHDwC642449MzIHwBAR4z8AeuSkRmA1WHkDwCgI0b+AFi0zTQiu1lspn1ilHxtGPkDAOiI8AcA0BE/+wIA64KfsNeGkT8AgI4IfwAAHRH+AAA6IvwBAHRE+AMA6IjwBwDQEZd6AVhlm+nyFcDGt6FG/qrqgKr6i6r6WlVdX1Xbq+pVVbX/rNsGALARbJiRv6q6Y5Kzk/xIkncl+XyS+yR5RpKHVtUDWmvfnGETAQDWvY008vd/MgS/p7fWjmytPa+1dkSSVya5c5I/mGnrAAA2gA0R/sZRv4ck2Z7kdROzfz/JNUkeX1X7rHHTAAA2lA0R/pI8aJx+oLX2/fkzWmvfTvKxJHsnud9aNwwAYCPZKMf83Xmcnj9l/hczjAwelOSDO1tRVZ0zZdbdzzvvvBxyyCHLa+EiXfwfV67q+gGA2TvkH35vVdd/3nnnJcnW5dTdKOFvv3E6LTnNld9qN7bxvR07dlx57rnnbt+NdezKXcbp51dxG+uVvut7b3rue9J3//Vd33Pu11d9e1uTXLWcihsl/K2Y1trqDu3txNyo4yzbMCv6ru+zbsta67nvSd/913d9n3VbdmWjHPM3N7K335T5c+VXrH5TAAA2ro0S/r4wTg+aMv9O43TaMYEAAGTjhL8PjdOHVNV/anNV3TLJA5Jcm+Sf1rphAAAbyYYIf621LyX5QIaDG39nYvaLk+yT5OTW2jVr3DQAgA1lI53w8dsZbu/2mqp6cJLzktw3wzUAz0/yghm2DQBgQ6jW2qzbsGhV9RNJXpLkoUluk+TiJH+b5MWttW/Nsm0AABvBhgp/AADsng1xzB8AACtD+AMA6IjwBwDQEeEPAKAjwh8AQEeEPwCAjgh/a6CqDqiqv6iqr1XV9VW1vapeVVX7z7pti1FVt6mqJ1XV31bVBVW1o6qurKqPVtWvT95yb169Q6vq9Kq6fKzzmao6tqr22Mm2HllVZ47rv7qq/rmqjlm93i1PVf1KVbXx8aQpyyy5L1V1TFV9Ylz+yrH+I1enF4tXVQ8e9/8l43v4a1X1/qp6+ALLbqr9XlWPqKoPVNVXx/5cWFVvr6r7T1l+w/S/qo6qqhOr6iNVddX4fj5lF3XWpH+r/VlYSt+r6k5V9dyqOqOq/r2qvlNVX6+qd1XVg1ayH1W1R1U9c3xdd4yv8+lVdeju9nliO0ve9xP13zTvO/Cnpiyz5L5U1c2r6sVV9YWquq6qLq2qt1XVwcvp55RtLOd9v0cN/w+eVVXfmvdd8NaqOmhKnXW575MkrTWPVXwkuWOSrydpSd6Z5PgkZ4zPP5/kNrNu4yL68JSxvV9L8ldJ/ijJXyS5Yiw/LeM1I+fVeUySG5JcneTPk7xs7G9L8vYp23nqOP+yJK9L8sok/z6WnTDr12FeO39i7Pu3x7Y9aSX6kuSEcf6/j8u/Lsk3x7KnzrC/fzKvXX+W5A+TvDHJuUn+ZDPv9yR/PK9tbxo/v6cl+U6S7yf5lY3c/ySfHrfz7Qx3TWpJTtnJ8mvSv7X4LCyl70lOHef/vyR/muE78G/G16IlefpK9CNJJXl7bvz/4WXj63z1uK3HzGrfT9R91Ly6LclPrURfkuyZ5KNjnU+On7//m+S7Sa5Jct8Zve9vkeSD43L/kuRVGb4LTk6yPckjN9K+b60Jf6v9SPL+cWc+baL8FWP5G2bdxkX04Yjxw/5DE+W3S/KVsR+PnVe+b5JLk1yf5F7zyvfKcIu+luToiXVtTXLd+OHYOq98/yQXjHXuvw5ei0ryj0m+NH44bxL+ltOXJIeO5Rck2X9iXd8c17d1tfq1k/4+eWzXSUl+eIH5N9us+318f38vySVJfmRi3oPGtl24kfs/9uNO4/v68Ow8AK1J/9bqs7DEvj8hyT0WKH9ghj8Erk9y+93tR5L/Odb5WJK95pXfe9zGpUluudb7fqLebcfPxKlJzsz08LfkviT53bHO2zPv/5sMf3TMhe8fWk5/d6fvGQY9WpLfnDL/ZhPP1/W+b034W9VHhlG/luSiyTdskltmSPTXJNln1m3djT4+f+zjifPKfm0se8sCyx8xzvvwRPlLxvIXL1Bn6vpm0N9nZBjxOSzJtiwc/pbclyR/OZY/cYE6U9e3yn3dc/zC+XIWCH5L2U8bcb9nuHd4S/KuKfOvSvLtzdL/7DoArUn/ZvFZ2FXfd1H3A5n4A3i5/Uhy1lj+oAXqTF3fWvY/wy1VL8lwi9UzMz38LakvGYLYl8fyA5eyvtXse5J7jvNPXcI61/2+d8zf6po7FuQDrbXvz5/RWvt2hoS/d5L7rXXDVtB3x+kN88qOGKfvW2D5s5Jcm+TQqtpzkXXeO7HMTIzHnByf5NWttbN2suhy+rIe+//zGf7K/5sk36/h2LfnVtUzauHj3Tbbfv9ihlGd+1TVf5k/o6oOy/AH3D/OK95s/Z+0Vv3bSK9JsvB3YLLEflTVXhlGjK5N8pHF1FlrVfWEJEdmGAH75k6WW05f7pjkvyY5v7V20SLrrIVfHqd/XVX71XC89+9W1W9MO9YxG2DfC3+r687j9Pwp8784Thc8WHS9q6otSX51fDr/TT613621GzKMhG5JcodF1rk4wwjpAVW19242e1nGvp6c4Wfu5+9i8SX1par2SfLjSa4e50+a1fvk3uP0ugzHufx9hvD7qiRnV9WHq+q285bfVPu9tXZ5kucm+dEkn6uqP6uqP6qqt2UY7fmHJL85r8qm6v8CVr1/6/izsKCq+skkD87wn/ZZ88qX0487Jtkjw6EEk0FyWp01M/b11RlGyN61i8WX05f1+v/l3PfgT2Y43OfkDMc9/2mS86vqdTXvZKeNsu+Fv9W13zi9csr8ufJbrX5TVsXxSe6a5PTW2vvnlS+n34uts9+U+avt95LcI8kTWms7drHsUvuyXt8nPzJOn5PhJ4f/lmG0624Zws9hGY7NmbPp9ntr7VVJfjFDqHlykucleVyGg7hPaq1dOm/xTdf/CWvRv/X6WbiJcYTzrzIcHrGttfatebNX87W61ZT5q6aGKzq8JcOhSk9fRJXN1P+578FXZPiZ++AM34P/PUMY/O0kL5q3/Ibou/DHslTV05Mcl+GspMfPuDmrqqrum2G07+WttY/Puj1raO774YYkj26tfbS1dnVr7d+S/EKSryZ54JSfgDeFqvpfGc7uPSnDX+f7JDkkyYVJ/qqq/mR2rWNWxpGek5M8IMlbM5zZuZk9M8PJLU+eCLk9mPse/HySX2qtfX78HvxgkqMyHAP+rKr64Zm1cBmEv9W1q7/a58qvWP2mrJyqemqG4f/PZTg49fKJRZbT78XWmfaX0aoYf+79yww/RbxoF4vPWWpf1uv7ZG57/9Ja2z5/Rmvt2gxnsifJfcbpptnvSVJVh2e41MTftdae1Vq7sLV2bWvt3Azh9z+SHFdVcz9zbqr+L2At+rdePws/MAa/UzKMAL8tw+V+2sRiq/laXTFl/qoYr2H3B0ne3Fo7fZHVNk3/523v3a21782f0Vr71wyHO9wyw4hgskH6Lvytri+M02m/099pnE47xmHdqapjk5yY5LMZgt8lCyw2td9jmDoww2jShYusc/sMIy5fHUPHWrrF2KaDk1w376KmLcnvj8u8cSx71fh8SX1prV2TIUjcYpw/aVbvk7l+XDFl/twIwM0nlt8M+z1J5i7G+qHJGWN7PpHhO/QeY/Fm6/+kVe/fOv4sJEmq6mZJ/jrJ0RmuP/fLCx2jtcx+fCnDpYXuML6ei6mzFn46w0/bT5z//Td+Bz5wXOaLY9mR4/Pl9GW9/n+5pO/BjbLvhb/VNfefxkNq4i4YVXXLDD8ZXJvkn9a6YctRVc/NcLHKT2cIfpdOWfSMcfrQBeYdluEM57Nba9cvss7DJpZZS9dnuNDmQo9/GZf56Ph87ifh5fRlPfZ/7qKmPz35/h3ddZzOnZm3mfZ7MvyHlwxnPC9krvw743Sz9X/SWvVvXb4m4896b88w4veXSR4/ORI0YUn9aK1dl+F6iXtnOL52l3XWyPZM/w6c++P/7ePz7cmy+/KlDCfUHVRVBy6yzlqYO6P/rpMzxuM+54LZ9nmz1v++X6lrxnhMvd7Phr/I89jeF43t/VSSW+9i2X2TfCNLuxjsgVmnF/vdST+3ZeHr/C25L1m/F3l+19iuZ06UPyTDsS7fSrLfZtzvSf7HuP1Lkvz4xLyHjf3fkfEuPRu9/1ncRZ5XvX+z+Cwsou97JnnPuMybsogLDS+nH1nchX73Xet9v5N6Z2b3LvK870SdNbnI8xL3/T4ZRvK+k+Q+E/NeOtY9Y6Pt+xV9A3ks+MaZvL3bH+XG27t9IRvj9m7HjO29IcPI37YFHk+YqHNkbrwN1Jsy3CLsB7eBysTt4MY6Txvnr7vbfE15XbZlgfC33L4kefk4f/7tgC4by2Zye7ckB+TGu7j8Y4a7mpw27tvv5qYXtt00+z3DLyP/MLbjqgxnO/5xkr/LEPxakmds5P6P7T1pfLxv3OaX5pWdMIv+rcVnYSl9T/Lmcf43krw4C38HHr67/ch/vsXXeePru1q3d1vSvp+yjjMzPfwtuS8ZQvbHxjqfzHBFidW4vduS+p7hmqfXj4+/znCCz0fGel9PcqeNtO9bE/7W5JHhXrBvTnJxhr8evpzhWmn7z7pti2z/tvENubPHmQvUe0CS0zOMDu1I8m8ZzhrbYyfbelSSD2e45+I14xfAMbN+DXbxutwk/C23LxluI/XJcflvj/Vvct/INe7nbTMc5/nl8f17WYar/N9nyvKbZr8nuVmSYzMcmnHV+CV8aYZrHj5ko/d/EZ/t7bPq32p/FpbS99wYcnb22LYS/chwWaFnjq/rjvF1Pj3JobPe9wusY+51uUn4W25fMvz0+ZIM17a7PkPgfnuSn57x+/7uGf7w/UaG78GvJHl9kh9bqffwWu371trwVxoAAH1wwgcAQEeEPwCAjgh/AAAdEf4AADoi/AEAdET4AwDoiPAHANAR4Q8AoCPCHwBAR4Q/AICOCH8AAB0R/gAAOiL8AQB0RPgDAOiI8AcA0BHhDwCgI8IfAEBH/j+zfGzFRkMuFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 319
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc88150",
   "metadata": {},
   "source": [
    "### 3. Preparing data\n",
    "To make our training task easier:\n",
    "1. Keep the most frequent notes. This will remove uncommon notes. \n",
    "2. Break into pairs of input and output notes as X and Y for training.\n",
    "3. assign integer to each note. This is similar to tokenization for NLP. \n",
    "4. Split data into train and test set for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4643aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 frequent notes found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremy/anaconda3/envs/wavenet/lib/python3.7/site-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(f\"{len(frequent_notes)} frequent notes found\")\n",
    "\n",
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ad6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eecda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
    "\n",
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)\n",
    "\n",
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8df2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e02f8",
   "metadata": {},
   "source": [
    "### 4. Define model\n",
    "Define wavenet.  \n",
    "One key difference in the model is that residual connections are removed, because the training data is so small anyway - we should be able to reach convergence quite soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b55c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 21:01:16.733867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:16.733896: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           18200     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 182)               46774     \n",
      "=================================================================\n",
      "Total params: 273,294\n",
      "Trainable params: 273,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 21:01:17.559956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-21 21:01:17.560454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-21 21:01:17.561088: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561238: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561326: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561370: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561433: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-21 21:01:17.561452: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-21 21:01:17.561643: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afd7f371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-21 21:01:25.255624: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "492/492 [==============================] - 9s 17ms/step - loss: 4.3593 - val_loss: 4.1586\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.15864, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "492/492 [==============================] - 9s 18ms/step - loss: 3.8802 - val_loss: 3.9260\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.15864 to 3.92598, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.7052 - val_loss: 3.7704\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.92598 to 3.77042, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "492/492 [==============================] - 9s 18ms/step - loss: 3.5789 - val_loss: 3.7276\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.77042 to 3.72757, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 3.4891 - val_loss: 3.6268\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.72757 to 3.62685, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "492/492 [==============================] - 9s 17ms/step - loss: 3.4146 - val_loss: 3.5685\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.62685 to 3.56853, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.3524 - val_loss: 3.5356\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.56853 to 3.53560, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 3.2945 - val_loss: 3.4741\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.53560 to 3.47415, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.2444 - val_loss: 3.4559\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.47415 to 3.45590, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.2004 - val_loss: 3.4231\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.45590 to 3.42310, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.1577 - val_loss: 3.4050\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.42310 to 3.40504, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.1203 - val_loss: 3.3714\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.40504 to 3.37136, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 3.0918 - val_loss: 3.3454\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.37136 to 3.34538, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 3.0520 - val_loss: 3.3471\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.34538\n",
      "Epoch 15/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 3.0222 - val_loss: 3.3334\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.34538 to 3.33337, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.9982 - val_loss: 3.3148\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.33337 to 3.31476, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "492/492 [==============================] - 9s 17ms/step - loss: 2.9717 - val_loss: 3.2780\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.31476 to 3.27805, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.9464 - val_loss: 3.2920\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.27805\n",
      "Epoch 19/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.9252 - val_loss: 3.2841\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.27805\n",
      "Epoch 20/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.9058 - val_loss: 3.2547\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.27805 to 3.25471, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.8834 - val_loss: 3.2691\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3.25471\n",
      "Epoch 22/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.8696 - val_loss: 3.2466\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.25471 to 3.24660, saving model to best_model.h5\n",
      "Epoch 23/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.8530 - val_loss: 3.2417\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.24660 to 3.24165, saving model to best_model.h5\n",
      "Epoch 24/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.8351 - val_loss: 3.2267\n",
      "\n",
      "Epoch 00024: val_loss improved from 3.24165 to 3.22672, saving model to best_model.h5\n",
      "Epoch 25/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.8153 - val_loss: 3.2300\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3.22672\n",
      "Epoch 26/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.8001 - val_loss: 3.2225\n",
      "\n",
      "Epoch 00026: val_loss improved from 3.22672 to 3.22247, saving model to best_model.h5\n",
      "Epoch 27/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.7873 - val_loss: 3.2166\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.22247 to 3.21658, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.7776 - val_loss: 3.2080\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.21658 to 3.20797, saving model to best_model.h5\n",
      "Epoch 29/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.7659 - val_loss: 3.2023\n",
      "\n",
      "Epoch 00029: val_loss improved from 3.20797 to 3.20234, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.7594 - val_loss: 3.1869\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.20234 to 3.18687, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      "492/492 [==============================] - 8s 16ms/step - loss: 2.7424 - val_loss: 3.1940\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.18687\n",
      "Epoch 32/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.7303 - val_loss: 3.1865\n",
      "\n",
      "Epoch 00032: val_loss improved from 3.18687 to 3.18647, saving model to best_model.h5\n",
      "Epoch 33/50\n",
      "492/492 [==============================] - 9s 18ms/step - loss: 2.7169 - val_loss: 3.1781\n",
      "\n",
      "Epoch 00033: val_loss improved from 3.18647 to 3.17809, saving model to best_model.h5\n",
      "Epoch 34/50\n",
      "492/492 [==============================] - 10s 19ms/step - loss: 2.7139 - val_loss: 3.1715\n",
      "\n",
      "Epoch 00034: val_loss improved from 3.17809 to 3.17152, saving model to best_model.h5\n",
      "Epoch 35/50\n",
      "492/492 [==============================] - 9s 19ms/step - loss: 2.6971 - val_loss: 3.1747\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.17152\n",
      "Epoch 36/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6941 - val_loss: 3.1733\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 3.17152\n",
      "Epoch 37/50\n",
      "492/492 [==============================] - 9s 18ms/step - loss: 2.6815 - val_loss: 3.1638\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.17152 to 3.16376, saving model to best_model.h5\n",
      "Epoch 38/50\n",
      "492/492 [==============================] - 9s 19ms/step - loss: 2.6739 - val_loss: 3.1548\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.16376 to 3.15482, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      "492/492 [==============================] - 10s 19ms/step - loss: 2.6657 - val_loss: 3.1623\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.15482\n",
      "Epoch 40/50\n",
      "492/492 [==============================] - 9s 18ms/step - loss: 2.6612 - val_loss: 3.1550\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.15482\n",
      "Epoch 41/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6540 - val_loss: 3.1543\n",
      "\n",
      "Epoch 00041: val_loss improved from 3.15482 to 3.15435, saving model to best_model.h5\n",
      "Epoch 42/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6448 - val_loss: 3.1490\n",
      "\n",
      "Epoch 00042: val_loss improved from 3.15435 to 3.14900, saving model to best_model.h5\n",
      "Epoch 43/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6422 - val_loss: 3.1526\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.14900\n",
      "Epoch 44/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6331 - val_loss: 3.1573\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.14900\n",
      "Epoch 45/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6231 - val_loss: 3.1473\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.14900 to 3.14735, saving model to best_model.h5\n",
      "Epoch 46/50\n",
      "492/492 [==============================] - 9s 17ms/step - loss: 2.6219 - val_loss: 3.1459\n",
      "\n",
      "Epoch 00046: val_loss improved from 3.14735 to 3.14586, saving model to best_model.h5\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6136 - val_loss: 3.1379\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.14586 to 3.13794, saving model to best_model.h5\n",
      "Epoch 48/50\n",
      "492/492 [==============================] - 8s 17ms/step - loss: 2.6100 - val_loss: 3.1412\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 3.13794\n",
      "Epoch 49/50\n",
      "492/492 [==============================] - 9s 18ms/step - loss: 2.6033 - val_loss: 3.1507\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 3.13794\n",
      "Epoch 50/50\n",
      "492/492 [==============================] - 9s 19ms/step - loss: 2.6037 - val_loss: 3.1302\n",
      "\n",
      "Epoch 00050: val_loss improved from 3.13794 to 3.13016, saving model to best_model.h5\n"
     ]
    }
   ],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4318009",
   "metadata": {},
   "source": [
    "### 5. Infer from best model\n",
    "Run best model on a random song from the validation set and play 10 timesteps from the song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053bd607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-22 00:30:34.786163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:34.786251: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-10-22 00:30:37.312779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-22 00:30:37.313534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-22 00:30:37.314368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314883: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314961: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-10-22 00:30:37.314974: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-22 00:30:37.315256: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce58b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 129, 46, 46, 46, 46, 46, 46, 58, 149]\n",
      "['G1', 'F2', '7.9.1', '7.9.1', '7.9.1', '7.9.1', '7.9.1', '7.9.1', '3.4', 'C3']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(10):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]\n",
    "    \n",
    "print(predictions)\n",
    "print(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a152556e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<music21.chord.Chord F# B->,\n",
       " <music21.note.Note E>,\n",
       " <music21.note.Note E>,\n",
       " <music21.note.Note E>,\n",
       " <music21.chord.Chord F# B->,\n",
       " <music21.chord.Chord F A>,\n",
       " <music21.chord.Chord F# B->,\n",
       " <music21.chord.Chord F A>,\n",
       " <music21.note.Note G>,\n",
       " <music21.chord.Chord F A>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv592624'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv592624');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCIgA/y8ATVRyawAAAJgA/wMAAOAAQIgAkEJaAJBGWogAgEIAAIBGAACQTFqIAIBMAACQTFqIAIBMAACQQFqIAIBAAACQQloAkEZaiACAQgAAgEYAAJBBWgCQRVqIAIBBAACARQAAkEJaAJBGWogAgEIAAIBGAACQQVoAkEVaiACAQQAAgEUAAJBbWogAgFsAAJBBWgCQRVqIAIBBAACARQCIAP8vAA==');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "    \"\"\"\n",
    "    Return output notes to play on notebook\n",
    "    \"\"\"\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')\n",
    "    \n",
    "    return output_notes\n",
    "\n",
    "output_notes = convert_to_midi(predicted_notes)\n",
    "display(output_notes)\n",
    "stream.Stream(output_notes).show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c4360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
